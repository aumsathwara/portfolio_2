---
title: "Understanding Model Context Protocol (MCP): A Beginner's Guide"
publishedAt: "2024-06-19"
summary: "Learn what Model Context Protocol is, how it works, and how to implement it in Python with practical server and client examples."
---

# Understanding Model Context Protocol (MCP)

Model Context Protocol (MCP) is an emerging standard designed to enhance interactions between AI applications and language models. If you've been working with AI systems or LLMs (Large Language Models), you've likely encountered limitations in how contextual information is shared between components. MCP aims to solve these problems through a standardized approach. In this guide, we'll explore what MCP is, how it works, and walk through Python examples for both server and client implementations.

## What is Model Context Protocol?

Model Context Protocol is a specification that standardizes how context information is passed between AI applications and language models. It's particularly valuable when working with tools, chains of reasoning, or any scenario where maintaining contextual awareness is important.

At its core, MCP defines a structure for:

1. Tracking conversation history
2. Managing tool usage and function calling
3. Providing metadata about the exchange
4. Maintaining state across multiple interactions

```python
# Example of basic MCP structure
{
    "messages": [],       # Conversation history
    "tools": [],          # Available tools
    "metadata": {},       # Additional context info
    "system": ""          # System instructions
}
```

The protocol helps solve common problems like context fragmentation, information loss between calls, and inconsistent handling of tool outputs across different LLM providers.

## How Model Context Protocol Works

MCP operates on a simple principle: encapsulate all context-related information in a standardized JSON structure that both client and server understand. This structure evolves as the conversation progresses.

### Key Components

| Component | Purpose | Example |
| --------- | ------- | ------- |
| `messages` | Stores conversation history | User queries, AI responses, tool calls |
| `tools` | Defines available functions | Search tools, calculators, data retrievers |
| `metadata` | Contains context-specific information | User preferences, session data |
| `system` | Provides system-level instructions | Behavior guidelines, restrictions |

The workflow typically follows these steps:

1. Client constructs an MCP object with initial context
2. Server (LLM) receives the context, processes it, and updates the MCP object
3. Client receives the updated context and can use it for subsequent requests
4. The cycle continues, maintaining a coherent conversation state

Let's see how this works in practice with some Python examples.

## Server Implementation Example

In this section, we'll build a basic MCP-compatible server using Python and FastAPI. This server will act as an intermediary between clients and an LLM.

```python
from fastapi import FastAPI, Request
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import openai
import os

app = FastAPI()

# MCP data models
class Message(BaseModel):
    role: str
    content: str
    
class Tool(BaseModel):
    name: str
    description: str
    parameters: Dict[str, Any]

class MCPRequest(BaseModel):
    messages: List[Message]
    tools: Optional[List[Tool]] = None
    metadata: Optional[Dict[str, Any]] = None
    system: Optional[str] = None

class MCPResponse(BaseModel):
    messages: List[Message]
    tools: Optional[List[Tool]] = None
    metadata: Optional[Dict[str, Any]] = None
    system: Optional[str] = None

# Initialize OpenAI client
openai_client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

@app.post("/chat", response_model=MCPResponse)
async def handle_chat(request: MCPRequest):
    # Convert MCP format to OpenAI format
    openai_messages = []
    
    # Add system message if provided
    if request.system:
        openai_messages.append({"role": "system", "content": request.system})
    
    # Add conversation history
    for msg in request.messages:
        openai_messages.append({"role": msg.role, "content": msg.content})
    
    # Convert tools if provided
    openai_tools = None
    if request.tools:
        openai_tools = [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.parameters
                }
            } for tool in request.tools
        ]
    
    # Call the LLM
    response = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=openai_messages,
        tools=openai_tools
    )
    
    # Extract response and update MCP object
    ai_message = response.choices[0].message
    
    # Add AI message to conversation history
    request.messages.append(
        Message(role="assistant", content=ai_message.content or "")
    )
    
    # Return updated MCP object
    return MCPResponse(
        messages=request.messages,
        tools=request.tools,
        metadata=request.metadata,
        system=request.system
    )
```

This server implementation:
1. Defines the MCP structure using Pydantic models
2. Converts between MCP format and the OpenAI API format
3. Maintains conversation history across requests
4. Supports tools for function calling

## Client Implementation Example

Now let's implement a client that interacts with our MCP server:

```python
import requests
import json
from typing import List, Dict, Any, Optional

class MCPClient:
    def __init__(self, server_url: str, system_prompt: Optional[str] = None):
        self.server_url = server_url
        self.context = {
            "messages": [],
            "tools": [],
            "metadata": {},
            "system": system_prompt
        }
    
    def add_tool(self, name: str, description: str, parameters: Dict[str, Any]):
        """Add a tool to the MCP context"""
        self.context["tools"].append({
            "name": name,
            "description": description,
            "parameters": parameters
        })
    
    def add_metadata(self, key: str, value: Any):
        """Add metadata to the MCP context"""
        self.context["metadata"][key] = value
    
    def chat(self, user_message: str) -> str:
        """Send a user message and get a response"""
        # Add user message to context
        self.context["messages"].append({
            "role": "user",
            "content": user_message
        })
        
        # Send request to server
        response = requests.post(
            f"{self.server_url}/chat",
            json=self.context
        )
        
        if response.status_code != 200:
            raise Exception(f"Error: {response.status_code} - {response.text}")
        
        # Update context with server response
        self.context = response.json()
        
        # Return the assistant's reply
        return self.context["messages"][-1]["content"]

# Example usage
if __name__ == "__main__":
    # Initialize client
    client = MCPClient(
        server_url="http://localhost:8000",
        system_prompt="You are a helpful AI assistant."
    )
    
    # Add a search tool
    client.add_tool(
        name="search",
        description="Search the internet for information",
        parameters={
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search query"
                }
            },
            "required": ["query"]
        }
    )
    
    # Add user metadata
    client.add_metadata("user_id", "user123")
    client.add_metadata("preferences", {"language": "English"})
    
    # Start conversation
    response = client.chat("Hello, can you help me find information about Python?")
    print(f"AI: {response}")
    
    # Continue conversation
    response = client.chat("Now tell me about data structures in Python.")
    print(f"AI: {response}")
```

This client:
1. Maintains the MCP context across multiple interactions
2. Allows adding tools and metadata
3. Handles the